{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audreyemmely/pln/blob/main/pln_lista6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdH0ObvpYJhx"
      },
      "source": [
        "# **Resolução Lista 6**\n",
        "\n",
        "---\n",
        "\n",
        "Audrey Emmely Rodrigues Vasconcelos\n",
        "\n",
        "Karen Nayara Gomes da Silva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49fJdXrr7jbZ"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#upando train_data e test_data_solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H55rLmyDBk81"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "import pathlib\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.models import Sequential, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTX97njjFMfF",
        "outputId": "a5f7a963-f0b5-417c-c177-3d96547d4835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iyLrIxQ7B3tL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train_data.txt', sep =':::', header = None, engine = 'python', nrows = 300)\n",
        "df.columns = ['id', 'title', 'genre', 'description']\n",
        "df2 = pd.read_csv('test_data_solution.txt', sep =':::', header = None, engine = 'python', nrows = 300)\n",
        "df2.columns = ['id', 'title', 'genre', 'description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eScG0RcwB4Pl"
      },
      "outputs": [],
      "source": [
        "df3 = pd.concat([df, df2], ignore_index=True)\n",
        "df3.drop('id', axis=1, inplace=True)\n",
        "comedy = df3.loc[df3['genre'].str.contains('comedy')]\n",
        "drama = df3.loc[df3['genre'].str.contains('drama')]\n",
        "dataset = pd.concat([comedy, drama], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ST8YriXuB-xD"
      },
      "outputs": [],
      "source": [
        "def preprocess(description):\n",
        "  description = re.sub(r'\\w*\\d\\w*', '', description) #remove todas as palavras que contêm números\n",
        "  description = re.sub(r'[^a-zA-Z ]', '', description.lower())\n",
        "  return re.sub(r'\\s+', ' ', description) #retira espaços repetidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JvOXznpOB_VU"
      },
      "outputs": [],
      "source": [
        "dataset['processed_description'] = dataset.description.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GxviXDHPCCec"
      },
      "outputs": [],
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "def remove_stopwords(description):\n",
        "    tokenized_text = nltk.word_tokenize(description, language='english')\n",
        "    return \" \".join([token for token in tokenized_text if token not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tg7JLq5NCEl2"
      },
      "outputs": [],
      "source": [
        "dataset['processed_description_stop'] = dataset.processed_description.apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "eiw7A6WDCGfT",
        "outputId": "f61557ac-7c08-4600-8337-8feacfccc033"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a8cf102e-8fdd-4785-a171-bf3ad61c4bc6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>genre</th>\n",
              "      <th>description</th>\n",
              "      <th>processed_description</th>\n",
              "      <th>processed_description_stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Pink Slip\" (2009)</td>\n",
              "      <td>comedy</td>\n",
              "      <td>In tough economic times Max and Joey have all...</td>\n",
              "      <td>in tough economic times max and joey have all...</td>\n",
              "      <td>tough economic times max joey run ideas discov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Babylon Vista (2001)</td>\n",
              "      <td>comedy</td>\n",
              "      <td>Frankie Reno was a child star on a TV show. B...</td>\n",
              "      <td>frankie reno was a child star on a tv show bu...</td>\n",
              "      <td>frankie reno child star tv show thirty years a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Söderpojkar (1941)</td>\n",
              "      <td>comedy</td>\n",
              "      <td>A gang of unemployed itinerant musicians play...</td>\n",
              "      <td>a gang of unemployed itinerant musicians play...</td>\n",
              "      <td>gang unemployed itinerant musicians play south...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tunnel Vision (1976)</td>\n",
              "      <td>comedy</td>\n",
              "      <td>A committee investigating TV's first uncensor...</td>\n",
              "      <td>a committee investigating tvs first uncensore...</td>\n",
              "      <td>committee investigating tvs first uncensored n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"The Young Professionals\" (2015)</td>\n",
              "      <td>comedy</td>\n",
              "      <td>Whether it's blocking up mouse holes, running...</td>\n",
              "      <td>whether its blocking up mouse holes running f...</td>\n",
              "      <td>whether blocking mouse holes running landlords...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>Future Weather (2012)</td>\n",
              "      <td>drama</td>\n",
              "      <td>When her single mom runs off to California, L...</td>\n",
              "      <td>when her single mom runs off to california la...</td>\n",
              "      <td>single mom runs california lauduree passionate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>Six Minutes to Midnight (????)</td>\n",
              "      <td>drama</td>\n",
              "      <td>Summer 1939. Influential families in Nazi Ger...</td>\n",
              "      <td>summer influential families in nazi germany h...</td>\n",
              "      <td>summer influential families nazi germany sent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>Jeunes filles en détresse (1939)</td>\n",
              "      <td>drama</td>\n",
              "      <td>Jacqueline is sixteen. Her parents are kept v...</td>\n",
              "      <td>jacqueline is sixteen her parents are kept ve...</td>\n",
              "      <td>jacqueline sixteen parents kept busy mutual ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Silence (2013/XI)</td>\n",
              "      <td>drama</td>\n",
              "      <td>'Silence' is an intimate portrait of the life...</td>\n",
              "      <td>silence is an intimate portrait of the life o...</td>\n",
              "      <td>silence intimate portrait life ten year old gi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>My Perfect Gentleman (????)</td>\n",
              "      <td>drama</td>\n",
              "      <td>Learn how one man struggles to find a way out...</td>\n",
              "      <td>learn how one man struggles to find a way out...</td>\n",
              "      <td>learn one man struggles find way poverty one w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>235 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8cf102e-8fdd-4785-a171-bf3ad61c4bc6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a8cf102e-8fdd-4785-a171-bf3ad61c4bc6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a8cf102e-8fdd-4785-a171-bf3ad61c4bc6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                  title  ...                         processed_description_stop\n",
              "0                   \"Pink Slip\" (2009)   ...  tough economic times max joey run ideas discov...\n",
              "1                 Babylon Vista (2001)   ...  frankie reno child star tv show thirty years a...\n",
              "2                   Söderpojkar (1941)   ...  gang unemployed itinerant musicians play south...\n",
              "3                 Tunnel Vision (1976)   ...  committee investigating tvs first uncensored n...\n",
              "4     \"The Young Professionals\" (2015)   ...  whether blocking mouse holes running landlords...\n",
              "..                                  ...  ...                                                ...\n",
              "230              Future Weather (2012)   ...  single mom runs california lauduree passionate...\n",
              "231     Six Minutes to Midnight (????)   ...  summer influential families nazi germany sent ...\n",
              "232   Jeunes filles en détresse (1939)   ...  jacqueline sixteen parents kept busy mutual ca...\n",
              "233                  Silence (2013/XI)   ...  silence intimate portrait life ten year old gi...\n",
              "234        My Perfect Gentleman (????)   ...  learn one man struggles find way poverty one w...\n",
              "\n",
              "[235 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLrZXVqZ7l5o"
      },
      "source": [
        "## Questão 1\n",
        "Resolva novamente a segunda questão da 3a lista usando pelo menos duas arquiteturas de redes neurais que utilizem camadas Embedding, convolucionais e \n",
        "LSTM. Compare com os resultados obtidos anteriormente nas lista 3 e 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpXvmJtZ8fAa"
      },
      "source": [
        "## Questão 2\n",
        "Usando sua base de textos:\n",
        "\n",
        "a) Treine uma rede LSTM para gerar texto, que receba uma ou mais palavras\n",
        "de uma frase como entrada. O treinamento deve ser realizado considerando\n",
        "um conjunto supervisionado que gera a próxima palavra de uma sequência\n",
        "de tamanho 4, usando subsequências de sua base.\n",
        "\n",
        "b) Após o treinamento, exiba pelo menos 5 exemplos de textos dados de entrada, e do texto gerado em seguida pela rede treinada. Para cada exemplo, gere pelo menos 10 palavras consecutivamente."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "def get_sequence_of_tokens(description):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(dataset.processed_description)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    input_sequences = []\n",
        "    for line in description:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(dataset.processed_description)"
      ],
      "metadata": {
        "id": "6bY-5WRO96DV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "metadata": {
        "id": "mRMtn4Sw-nUn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "  \n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c6dFGDE_5QL",
        "outputId": "b19ccbe4-8523-47cd-e227-616dd858cced"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 660, 10)           57310     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5731)              578831    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 680,541\n",
            "Trainable params: 680,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(predictors, label, epochs=11, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2UzSEHbAIc0",
        "outputId": "e0cfab6e-9aa1-4cfe-f654-ea422677694a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "750/750 [==============================] - 454s 597ms/step - loss: 7.2160\n",
            "Epoch 2/11\n",
            "750/750 [==============================] - 425s 567ms/step - loss: 6.7974\n",
            "Epoch 3/11\n",
            "750/750 [==============================] - 440s 586ms/step - loss: 6.6510\n",
            "Epoch 4/11\n",
            "750/750 [==============================] - 413s 551ms/step - loss: 6.4953\n",
            "Epoch 5/11\n",
            "750/750 [==============================] - 418s 558ms/step - loss: 6.3433\n",
            "Epoch 6/11\n",
            "750/750 [==============================] - 414s 553ms/step - loss: 6.2013\n",
            "Epoch 7/11\n",
            "750/750 [==============================] - 412s 549ms/step - loss: 6.0516\n",
            "Epoch 8/11\n",
            "750/750 [==============================] - 412s 549ms/step - loss: 5.8795\n",
            "Epoch 9/11\n",
            "750/750 [==============================] - 416s 554ms/step - loss: 5.7024\n",
            "Epoch 10/11\n",
            "750/750 [==============================] - 413s 551ms/step - loss: 5.5229\n",
            "Epoch 11/11\n",
            "539/750 [====================>.........] - ETA: 1:55 - loss: 5.3369"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len): \n",
        "    #seed_text: string para teste, next_word: qtdd de palavras q quer prever, max_sequence_len: contagem max de seq usada durante o treinamento\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        #predicted = model.predict_classes(token_list, verbose=0)\n",
        "        predicted = model.predict(token_list) \n",
        "        classes = np.argmax(predicted,axis=1)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == classes:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "metadata": {
        "id": "4SseLskbASO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (generate_text(\"frankie reno was a child\", 4, model, max_sequence_len))"
      ],
      "metadata": {
        "id": "xlpfz70-ATGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (generate_text(\"whether its blocking up\", 10, model, max_sequence_len))\n",
        "print (generate_text(\"science and technology\", 10, model, max_sequence_len))\n",
        "print (generate_text(\"awake from a deep sleep\", 10, model, max_sequence_len))\n",
        "print (generate_text(\"first film\", 10, model, max_sequence_len))\n",
        "print (generate_text(\"the sydney opera house\", 10, model, max_sequence_len))"
      ],
      "metadata": {
        "id": "8Tfhm_6VFrez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questão 3\n",
        "Usando sua base de textos e a biblioteca spaCy, realize as seguintes tarefas:\n",
        "\n",
        "a) Extraia as etiquetas gramaticais (POS) de cada token do seu textos.\n",
        "\n",
        "b) Calcule e plote um gráfico com as frequências de cada tipo gramatical.\n",
        "\n",
        "c) Extraia entidades do tipo pessoa e lugar dos seus textos.\n",
        "\n",
        "d) Identifique e liste as pessoas mais frequentes nos seus textos. Você só deve contar cada entidade 1 vez por documento."
      ],
      "metadata": {
        "id": "EU7F2228KdPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "YtLG_xuW36Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_description_list = list(dataset['processed_description_stop'].values)"
      ],
      "metadata": {
        "id": "7g8GsjbM_JlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_pos = []\n",
        "\n",
        "for phrase in processed_description_list:\n",
        "    doc = nlp(phrase)\n",
        "\n",
        "    for token in doc:        \n",
        "        token_pos.append(token.pos_)\n",
        "\n",
        "token_pos[:20]"
      ],
      "metadata": {
        "id": "Hnn-0E4N_gAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(24,8))\n",
        "plt.hist(token_pos, bins=16, rwidth=0.5, align='left', color='green')\n",
        "\n",
        "plt.xlabel('Tipo gramatical')\n",
        "plt.ylabel('Quantidade')\n",
        "  \n",
        "plt.title('Frequência de cada tipo gramatical',\n",
        "          fontweight =\"bold\")\n",
        "  \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pE95g_vQ6Izh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities_person_gpe = []\n",
        "persons = {}\n",
        "entities_to_search = ['PERSON', 'GPE']\n",
        "\n",
        "for phrase in processed_description_list:\n",
        "    doc = nlp(phrase)\n",
        "\n",
        "    for entity in doc.ents:\n",
        "      entity_persons_in_doc = []\n",
        "\n",
        "    if entity.label_ in entities_to_search:\n",
        "      entities_person_gpe.append((entity.text, entity.label_))\n",
        "      \n",
        "      if entity.label_ == 'PERSON':\n",
        "          entity_persons_in_doc.append(entity.text)\n",
        "      \n",
        "      unique_persons = list(set(entity_persons_in_doc))\n",
        "      \n",
        "      for unique_person in unique_persons:\n",
        "        if unique_person in persons.keys():\n",
        "          persons[unique_person] += 1\n",
        "        else:\n",
        "          persons[unique_person] = 1"
      ],
      "metadata": {
        "id": "bQGbRbWzDSlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities_person_gpe[:20]"
      ],
      "metadata": {
        "id": "5q-uPX57F9Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_entities = list([ent[1] for ent in entities_person_gpe])\n",
        "\n",
        "[(entity, get_entities.count(entity)) for entity in set(get_entities)]"
      ],
      "metadata": {
        "id": "YAtkTOezGzZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent_persons = [(key, value) for key, value in persons.items()]\n",
        "most_frequent_persons.sort(key= lambda person : person[1], reverse=True)\n",
        "most_frequent_persons[:10]"
      ],
      "metadata": {
        "id": "abUUExtxJS20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questão 4\n",
        "Estude o tutorial *Character-level recurrent sequence-to-sequence model* disponível em https://keras.io/examples/nlp/lstm_seq2seq/.\n",
        "\n",
        "a) Treine um outro modelo de tradução entre línguas distintas e exiba 5 exemplos de tradução de frases curtas. Você pode encontrar conjuntos de treinamento em http://www.manythings.org/anki/.\n",
        "\n",
        "b) BONUS: Adapte o código para realizar tradução com uma rede *word-level*.\n"
      ],
      "metadata": {
        "id": "Qen9foNaKfnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "OONCI9dSctgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "metadata": {
        "id": "cgeHZCxvcxi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "oX97IEuZc1rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vetorizar os dados**"
      ],
      "metadata": {
        "id": "elh2XNt7c71_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "cEzsuvNBdAv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "LzSjDfIgdPvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "-NER61Ltdbk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construir o modelo**"
      ],
      "metadata": {
        "id": "uNf2pqVWdhZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "IVRSFNAPdp90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")"
      ],
      "metadata": {
        "id": "90Zl8Hqzdyox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treinando o modelo**"
      ],
      "metadata": {
        "id": "jVd7vH-feLN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demorou cerca de 1 hora pra rodar (por causa do epochs)\n",
        "transformer.summary()\n",
        "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=1, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "cIaGqvtWeM-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decodifica as sentanças**"
      ],
      "metadata": {
        "id": "tq4o3AQMeYgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(20):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence.replace('[start] ', '').replace(' [end]', '')\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(str(i + 1) + '.')\n",
        "    print('Input sentence:', input_sentence)\n",
        "    print('Decoded sentence:', translated)"
      ],
      "metadata": {
        "id": "7Z4-2wXaeb2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkZ2rKF1CnNO"
      },
      "source": [
        "**Referências**\n",
        "\n",
        "[Beginners Guide to Text Generation using LSTMs](https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms)\n",
        "\n",
        "[English-to-Spanish translation with a sequence-to-sequence Transformer](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pln_lista6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}